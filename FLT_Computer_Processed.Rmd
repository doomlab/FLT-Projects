---
title: "FLT Text Processor"
author: "Erin M. Buchanan"
date: "11/22/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Libraries

```{r}
library(stringr)
library(data.table)
library(stringi)
library(dplyr)
library(rvest)
library(tidytext)
library(textclean)
```


## Read in the Files

- This section reads in the raw text files of the 3722 words we collected for the Buchanan et al. 2013 and 2019 papers. We will combine these with the other norms later for the full dataset. 

- If you check out https://github.com/doomlab/Word-Norms-2 you can download these files for yourself - this folder is the .5 raw words folder. The 0 word lists folder has the data in a bunch of different formats. This folder has the data in single text files for each word. 

- Please note the `word_con.txt` file is the word `con` but you can't name a file `con.txt` on windows machines. 

```{r}
##find the files
the_file_path <- "/Users/buchanan/GitHub/46 Word-Norms-2/0.5 raw words"
list.filenames <- list.files(path = the_file_path)

##create empty list to store stuff
list.data <- list()

##open all the files 
for (i in 1:length(list.filenames)){
  list.data[[i]] <- readLines(paste(the_file_path, "/", list.filenames[i], sep = ""))
}

##give them names
names(list.data) <- list.filenames

##create the final data frame
final_data <- data.frame(unlist(list.data))
final_data$cue <- rownames(final_data)
colnames(final_data)[1] <- "participant_answer"
final_data$participant_answer <- as.character(final_data$participant_answer)

##clean up the cue names
##find participant number with the file number that is appended 
final_data$participant_num <- str_extract(final_data$cue, 
                                          pattern = "\\d+")

##take the numbers .txt out of the cue
final_data$cue <- str_replace_all(final_data$cue,
                                  pattern = "\\d+|.txt|word_",
                                  replacement = "")
```

## Remove symbols

- Practically, we need to remove symbols first, as they interfere with the ability of processing the rest of the data (i.e., counting, searching, etc.). 

```{r}
final_data$participant_answer <- stri_trans_general(str = final_data$participant_answer,
                   id = "Latin-ASCII")
```

## Remove copied articles

- When using MTurk participants, we had several participants who copied from Wikipedia and/or Synonyms Pages as their answer. 

```{r}
final_data$character_num <- nchar(final_data$participant_answer)
hist(final_data$character_num, breaks = 100)

bad_character_count = 150

##remove all empties
no_zero <- subset(final_data, character_num > 0)
nrow(final_data) - nrow(no_zero)

##remove all duplicates over bad_character_count characters
no_zero$duplicate <- duplicated(no_zero$participant_answer) & no_zero$character_num > bad_character_count

no_duplicates <- subset(no_zero, duplicate == FALSE)
nrow(no_zero) - nrow(no_duplicates)

##remove long answers that start with " 1) or (1) over bad_character_count characters
no_duplicates$problem <-  str_detect(no_duplicates$participant_answer, 
           pattern = "^1\\)|^\\(1\\)|^\"") & no_duplicates$character_num > bad_character_count
no_wiki <- subset(no_duplicates, problem == FALSE)
nrow(no_duplicates) - nrow(no_wiki)

write.csv(no_wiki, "./output_data/wiki_to_check.csv", row.names = F)
```

## Check for copy and paste

- This analysis checks for plagiarism in participant answers with at least 200 characters. Because we are checking with an online tool, these were run in batches slowly to not crash or upset the IP address holders. 
- This analysis does not run when you knit, but could be used code-wise for other entries.

```{r plag_check, eval = F}

no_wiki <- read.csv("./output_data/wiki_to_check.csv", 
                          stringsAsFactors = F)

#wiki_to_check <- subset(no_wiki, character_num > 400)
wiki_to_check <- subset(no_wiki, character_num > 300 & character_num <=400)

library(RSelenium)

#open firefox 
rD <- rsDriver(browser = "firefox")

##connect the driver client
remDr <- rD[["client"]]

##create a place to store the answers
#wiki_to_check$plag_check <- NA

for (i in 1:nrow(wiki_to_check)){ 

  ##go to plag_detector 
  remDr$navigate("https://freeplagiarismchecker.pro")
  
  ##find the typing box
  webElem <- remDr$findElement(using = "name","text")
  
  ##put in your words you want to translate 
  webElem$sendKeysToElement(list(wiki_to_check$participant_answer[i], "\uE007"))
  
  ##find the button 
  webElem <- remDr$findElement(using = "class name","btn-start-check")
  
  ##click the button
  webElem$clickElement()
  
  #pause so the page doesn't freak out 
  Sys.sleep(2)
  
  ##read in the answers
  webpage <-remDr$getPageSource()
  
  ##find the number
  answers <- webpage %>% 
    unlist() %>% 
    read_html() %>% 
    html_nodes("div") %>% 
    html_text() %>% 
    str_trim()

  ##save the answer
  wiki_to_check$plag_check[i] <- answers[str_detect(answers, "^\\d+%|^0\n\nSimilarities")][1]
  
  ##sys sleep to pause
  Sys.sleep(runif(1,10,11))
}

## close the remote after you are done, make sure to give everything above time to run
remDr$close()
# stop the selenium server
rD[["server"]]$stop()

write.csv(wiki_to_check, "./output_data/screened_answers_300.csv", row.names = F)

#write.csv(wiki_to_check, "./output_data/screened_answers_400.csv", row.names = F)
```

## Clear out bad entries

- These saved files were updated to fix the ~ seven entries that came up as NA in the web scraping. 
- We will reimport them and use them to do a final wikipedia/dictionary filter. 

```{r}
checked_wiki <- rbind(
  read.csv("./output_data/screened_answers_300.csv", stringsAsFactors = F),
  read.csv("./output_data/screened_answers_400.csv", stringsAsFactors = F))

checked_wiki$percent <- str_extract(checked_wiki$plag_check, 
                                   pattern = "\\d+")
checked_wiki$where <- str_detect(checked_wiki$plag_check, 
                                 pattern = "wiki|quizlet|diction|answer|course")

#anything marked true
table(checked_wiki$where)
#anything over 50% 
checked_wiki$percent <- as.numeric(checked_wiki$percent)
hist(checked_wiki$percent)
table(checked_wiki$percent > 50)

#pull out of the original data 
bad_answers <- subset(checked_wiki, 
                      where == TRUE | percent > 50)
nrow(bad_answers)

#no wiki is the last real dataset before checking section
no_bad_answers <- subset(no_wiki,
                         !(participant_answer %in% bad_answers$participant_answer))

nrow(no_wiki) - nrow(no_bad_answers)
```

## Expand to long format 

- This section drops the duplicate and problem columns that were used to check for wikipedia and other issues.
- Then, we use unnest_tokens to separate out participants answers from one large dataset until more traditional cue-feature responses (no perfect but some good guesses based on tabs, commas, and semi-colons).
- Finally, extra spaces are trimmed out. 

```{r}
no_bad_answers <- 
  no_bad_answers %>% 
  select(-duplicate, -problem, -character_num) %>% 
  unnest_tokens(output = participant_answer, 
                input = participant_answer, 
                token = stringr::str_split, 
                        pattern = "  |\\, |\\.|\\,|\\;")
no_bad_answers$participant_answer <- trimws(no_bad_answers$participant_answer, 
                       which = c("both", "left", "right"), 
                       whitespace = "[ \t\r\n]")

##remove empty answers
no_bad_answers <- subset(no_bad_answers, 
                         nchar(participant_answer) > 0)
```

## Expand contractions

- Next, we will expand all contractions using the contractions dictionary in `textclean`. You can edit this dictionary if interested or we are missing some. 

```{r}
##fix any issues with the type of contraction symbol
no_bad_answers$participant_answer <- str_replace_all(no_bad_answers$participant_answer, 
                                                     pattern = "â€™", 
                        replacement = "'")

##lower case just in case
no_bad_answers$participant_answer <- tolower(no_bad_answers$participant_answer)

##expand the contractions lexicon 
contractions <- lexicon::key_contractions
contractions$contraction <- tolower(contractions$contraction)
contractions$expanded <- tolower(contractions$expanded)

##find all ' with word boundaries 
used_apostrophe <- unique(unlist(str_extract_all(no_bad_answers$participant_answer, 
                pattern = "\\b\\w+\'\\w+\\b",
                simplify = F)))

used_apostrophe <- used_apostrophe[!(used_apostrophe %in% contractions$contraction)]

##add contractions
new_contract <- c("b'day", "dip'n", "doens't", "does'nt", "does't", 
                  "dosn't", "gov't", "hadn't", "haven't", "needn't", 
                  "rock'n", "	theyr'e", "toys'r", "y'all", "your'e")
new_expand <- c("birthday", "dip and", "does not", "does not", "does not", 
                "does not", "government", "had not", "have not", "need not",
                "rock and", "they are", "toys r", "you all", "you are")
new_together <- cbind(new_contract, new_expand)
colnames(new_together) <- colnames(contractions)
contractions <- rbind(contractions, new_together)
write.csv(contractions, "./output_data/contraction_dict.csv", row.names = F)

##replace contractions
no_contractions <- no_bad_answers
no_contractions$participant_answer <- replace_contraction(no_contractions$participant_answer, 
                                       contraction.key = contractions,
                                       ignore.case = T)

##take off 's possessives
no_contractions$participant_answer <- str_replace_all(no_contractions$participant_answer, 
                                                      pattern = "\\'s", 
                                                      replacement = "")

##replace all other symbols
no_contractions$participant_answer <- str_replace_all(no_contractions$participant_answer, 
                                                      pattern = "[:punct:]", 
                                                      replacement = "")

##write it out
write.csv(no_contractions, "./output_data/text_no_contractions.csv", row.names = F)
```

## Spelling

```{r}
#can read in the no_contractions file if you need to

# Extract a list of words
tokens <- unnest_tokens(tbl = no_contractions, output = token, input = participant_answer)
wordlist <- unique(tokens$token)
# Spell check the words
spelling.errors <- hunspell(wordlist)
spelling.errors <- unique(unlist(spelling.errors))
spelling.sugg <- hunspell_suggest(spelling.errors, dict = dictionary("en_US"))
```


## Lemmatize

## Remove Stopwords

## Bag of Words processing 